{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import pprint as pp\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - Filter and Clean Data](#1)\n",
    "- [2 - Rollup Class](#2)\n",
    "  - [2.1 - Deploy Rollup](#2-1)\n",
    "- [3 - Read in and Filter Supply Data](#3)\n",
    "- [4 - NMCS Class](#4)\n",
    "  - [4.1 - Run NMCS Calculator Class](#4-1)\n",
    "- [5 - Final Join and Rollup for NMC and NMCS results](#5)\n",
    "  - [5.1 - Deploy Join Class](#5-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Filter and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (34,35,54,58,59,60,62,67,68,69,70,71,72,73,76,77,81,82,83) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Evt_ID</th>\n",
       "      <th>Wuc</th>\n",
       "      <th>location</th>\n",
       "      <th>usmcsymbol</th>\n",
       "      <th>lewdId</th>\n",
       "      <th>Start_Date</th>\n",
       "      <th>Stop_Date_Time</th>\n",
       "      <th>hours</th>\n",
       "      <th>NMCHours</th>\n",
       "      <th>PMCHours</th>\n",
       "      <th>End_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0025</td>\n",
       "      <td>540000</td>\n",
       "      <td>Cannon</td>\n",
       "      <td>NMC</td>\n",
       "      <td>5417-A220530101-1-1</td>\n",
       "      <td>2022-02-22</td>\n",
       "      <td>2022-02-23 09:00:00</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0027</td>\n",
       "      <td>720000</td>\n",
       "      <td>Cannon</td>\n",
       "      <td>NMC</td>\n",
       "      <td>5417-A220320042-1-1</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>2022-02-01 02:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0027</td>\n",
       "      <td>270000</td>\n",
       "      <td>Cannon</td>\n",
       "      <td>NMC</td>\n",
       "      <td>5417-A220450238-1-1</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>2022-02-23 06:00:00</td>\n",
       "      <td>227.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0027</td>\n",
       "      <td>219000</td>\n",
       "      <td>Cannon</td>\n",
       "      <td>NMC</td>\n",
       "      <td>5417-A220460041-1-1</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>2022-02-15 01:15:00</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-02-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0027</td>\n",
       "      <td>219000</td>\n",
       "      <td>Cannon</td>\n",
       "      <td>NMC</td>\n",
       "      <td>5417-A220460041-2-1</td>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>2022-02-15 01:20:00</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-02-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Evt_ID     Wuc location usmcsymbol               lewdId  Start_Date  \\\n",
       "0  A0025  540000   Cannon        NMC  5417-A220530101-1-1  2022-02-22   \n",
       "1  A0027  720000   Cannon        NMC  5417-A220320042-1-1  2022-02-01   \n",
       "2  A0027  270000   Cannon        NMC  5417-A220450238-1-1  2022-02-14   \n",
       "3  A0027  219000   Cannon        NMC  5417-A220460041-1-1  2022-02-15   \n",
       "4  A0027  219000   Cannon        NMC  5417-A220460041-2-1  2022-02-15   \n",
       "\n",
       "        Stop_Date_Time  hours  NMCHours  PMCHours    End_Date  \n",
       "0  2022-02-23 09:00:00   38.0      38.0       0.0  2022-02-23  \n",
       "1  2022-02-01 02:00:00    7.0       7.0       0.0  2022-02-01  \n",
       "2  2022-02-23 06:00:00  227.0     227.0       0.0  2022-02-23  \n",
       "3  2022-02-15 01:15:00    6.2       6.2       0.0  2022-02-15  \n",
       "4  2022-02-15 01:20:00    6.3       6.3       0.0  2022-02-15  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in IMDS data\n",
    "# df = pd.read_csv('newCV22AFMaint3.csv')\n",
    "df = pd.read_csv('lukeFeb.csv')\n",
    "\n",
    "#filter to appropriate data\n",
    "cols = ['EVT.EQUIP.ID','WCE.WUC.LCN.UNS','location','usmcsymbol','lewdId','Start_Date_0000','Stop_Date_Time','hours','NMCHours','PMCHours']\n",
    "df = df[cols]\n",
    "\n",
    "#Delete rows in which hours is negative\n",
    "df = df.loc[df['NMCHours']>=0]\n",
    "\n",
    "#rename cols for interpreatbility\n",
    "df.rename({'Start_Date_0000':'Start_Date','EVT.EQUIP.ID':'Evt_ID','WCE.WUC.LCN.UNS':'Wuc'}, axis=1, inplace=True)\n",
    "\n",
    "df['End_Date'] = pd.to_datetime(df['Stop_Date_Time']).dt.date\n",
    "\n",
    "#remove all mafs where the start date is after end date\n",
    "date_issues = df.loc[df['Start_Date']>df['Stop_Date_Time']]\n",
    "df = df.loc[~(df['Start_Date']>df['Stop_Date_Time'])]\n",
    "\n",
    "#drop all rows where event ID is NA\n",
    "df.dropna(subset=['Evt_ID'], inplace=True)\n",
    "\n",
    "#filter to appropriate dates\n",
    "df = df.loc[df['Stop_Date_Time'] > '2021-12-01']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Rollup Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMC_Calculation():\n",
    "    '''\n",
    "    Rollup IMDS Data to calculate NMC Hours by Month and Lewd-ID.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, status_detail, oor_overlay, monthly = True):\n",
    "        \n",
    "        self.df = df\n",
    "        self.status_detail = status_detail\n",
    "        self.oor_overlay = oor_overlay\n",
    "        self.monthly = monthly\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.NMCOverlay()\n",
    "        self.ExpandDaily()\n",
    "        self.OOROverlay()\n",
    "        self.FinalRollup()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def NMCOverlay (self):\n",
    "        '''\n",
    "        Overlay status detail to determine proper start date for each record.\n",
    "        '''\n",
    "        \n",
    "        nmc = pd.read_csv(self.status_detail)\n",
    "        nmc = nmc.iloc[:,1:]\n",
    "    \n",
    "        #convert our start and stop dates to datetime\n",
    "        nmc['StartDateTime'] = pd.to_datetime(nmc['StartDateTime'])\n",
    "        nmc['stop'] = pd.to_datetime(nmc['stop'])\n",
    "        \n",
    "        \n",
    "        #match our status detail to IMDS by status detail and evt_id\n",
    "        nmc['match'] = nmc['SerNum'].astype(str).str[-2:]\n",
    "        self.df['match'] = self.df['Evt_ID'].astype(str).str[-2:]\n",
    "        \n",
    "        \n",
    "        #Determine new start date base on NMC Flag\n",
    "        \n",
    "        #zip columns we need to compare to nmc columns\n",
    "        self.df['zipped'] = [[i,j] for i,j in zip(self.df['match'], self.df['Stop_Date_Time'])]\n",
    "\n",
    "        def adjust_nmc_start(x):\n",
    "            match, End = x\n",
    "            pdEnd = pd.to_datetime(End)\n",
    "\n",
    "\n",
    "            match_idx = nmc.loc[(nmc['match']==match) & (nmc['StartDateTime']<pdEnd) & (nmc['stop']>pdEnd)].index\n",
    "            if len(match_idx)>1:\n",
    "                raise ValueError('Got more than one row.')\n",
    "            elif len(match_idx)==0:\n",
    "                return 'Okay'\n",
    "\n",
    "\n",
    "            if nmc.iloc[match_idx]['NMCFlag'].values[0] == 0:\n",
    "                return [(i,j,k) for i,j,k in zip(nmc.iloc[match_idx-2]['match'],nmc.iloc[match_idx-2]['NMCFlag'],nmc.iloc[match_idx-2]['stop'])]\n",
    "            return [(i,j,k) for i,j,k in zip(nmc.iloc[match_idx-1]['match'],nmc.iloc[match_idx-1]['NMCFlag'],nmc.iloc[match_idx-1]['stop'])]\n",
    "\n",
    "        \n",
    "        self.df['nmc_zipped'] = self.df['zipped'].apply(lambda x: adjust_nmc_start(x))\n",
    "        self.df['nmc_zipped'] = self.df['nmc_zipped'].apply(lambda x: x[0])\n",
    "   \n",
    "        # if a zipped match or nmc flag disagrees with our expectations, or is 'Okay', label them as 'Ok' to indicate using our original start date, and not zipped 'EndDate'\n",
    "        self.df.loc[(self.df['nmc_zipped'].str[1]==1) | (self.df['nmc_zipped'].str[0]!=self.df['match'].astype(str)), 'nmc_zipped'] = 'Ok'\n",
    "        self.df.loc[self.df['nmc_zipped']!='Ok','Start_Date'] = self.df['nmc_zipped'].apply(lambda x:x[-1])\n",
    "        \n",
    "        #drop our zipped columns\n",
    "        self.df.drop([col for col in self.df.columns if 'zipped' in col],axis=1,inplace=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def ExpandDaily(self):\n",
    "        '''\n",
    "        Expand each record to daily records between its start and end date, and calculate daily hours.\n",
    "        '''\n",
    "        #pp.pprint(self.df.head())\n",
    "        # get Start date from new start date\n",
    "        self.df.rename({'Start_Date':'Start_Date_Time'},axis=1, inplace=True)\n",
    "        self.df = self.df.iloc[:,0:11]\n",
    "\n",
    "        self.df['Start_Date'] = pd.to_datetime(self.df['Start_Date_Time']).dt.date\n",
    "        \n",
    "#         #avoid duplicate keys\n",
    "#         if isinstance(self.df['Start_Date'], pd.DataFrame):\n",
    "#             self.df['Start_Date'] = self.df['Start_Date'].astype(str).apply(\n",
    "#                 pd.to_datetime, format='%Y-%m-%d', errors='coerce'\n",
    "#             )\n",
    "#         else:\n",
    "#             self.df['Start_Date'] = pd.to_datetime(self.df['Start_Date_Time']).dt.date\n",
    "        \n",
    "        \n",
    "        def drange(s, e):\n",
    "            s = str(s)\n",
    "            e = str(e)\n",
    "\n",
    "            return pd.Series(pd.date_range(s,e))\n",
    "\n",
    "        self.df['Date'] = [pd.date_range(s, e, freq='d') for s, e in\n",
    "              zip(pd.to_datetime(self.df['Start_Date']), pd.to_datetime(self.df['End_Date']))]\n",
    "        self.df = self.df.explode('Date')\n",
    "        \n",
    "        #Calculate daily hours\n",
    "        self.df['Datetime'] = self.df['Date'].astype(str)+' 00:00:00'\n",
    "        self.df['Date'] = pd.to_datetime(self.df['Datetime']).dt.date\n",
    "        self.df['end_bool'] = np.where(self.df['Date']==self.df['End_Date'], 'Yes','No')\n",
    "        \n",
    "        def daily_hour(row):\n",
    "            if pd.to_datetime(row['Date'])==row['Start_Date']:\n",
    "                return 24 - (int(str(row['Start_Date_Time'])[-8:-6]) + float(str(row['Start_Date_Time'])[-5:-3])/60)\n",
    "            elif row['end_bool']=='Yes':\n",
    "                return  int(str(row['Stop_Date_Time'])[-8:-6]) + (float(str(row['Stop_Date_Time'])[-5:-3])/60)\n",
    "            else:\n",
    "                return 24\n",
    "\n",
    "        self.df['daily_hours'] = self.df.apply(lambda x: daily_hour(x), axis=1)\n",
    "        self.df.drop('end_bool',axis=1,inplace=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def OOROverlay(self):\n",
    "        '''\n",
    "        Determine whether aircraft went into OOR between start and end and substract that time from daily total. \n",
    "        '''\n",
    "        \n",
    "        oor = pd.read_csv(self.oor_overlay)\n",
    "        oor = oor.iloc[:,1:]\n",
    "        \n",
    "        #create column based on last two numbers for 'SerNum' and 'EVT.EQUIP.ID' to overlay oor\n",
    "        oor['match'] = oor['SerNum'].astype(str).str[-2:]\n",
    "        self.df['match'] = self.df['Evt_ID'].astype(str).str[-2:]\n",
    "        \n",
    "        #filter out IR statuses in our oor df\n",
    "        oor_df = oor.loc[oor['ReportingStatus']=='OOR']\n",
    "        \n",
    "        oor_df['StartDate'] = pd.to_datetime(oor_df['StartDateTime'],  errors='coerce')\n",
    "        oor_df['EndDate'] = pd.to_datetime(oor_df['StopDateTime'],  errors='coerce')\n",
    "        \n",
    "        \n",
    "        for i, row in oor_df.iterrows():\n",
    "            self.df.loc[(self.df['match'] == row['match']) & (self.df['Date'].between(row['StartDate'], row['EndDate'])), 'oor_flag'] = 1\n",
    "\n",
    "        self.df['oor_flag'].fillna(0, inplace=True)\n",
    "        self.df['oor_flag'] = self.df['oor_flag'].astype(int)\n",
    "        \n",
    "        #remove days that are flagged as OOR\n",
    "        self.df = self.df.loc[self.df['oor_flag']!=1]\n",
    "        \n",
    "    \n",
    "    def FinalRollup(self):\n",
    "        '''\n",
    "        Depending on the keyword (monthly), we will roll up the data by month or lewd-id.\n",
    "        '''\n",
    "        \n",
    "        #get first of every month to roll up to\n",
    "        self.df['TransDate'] = self.df['Date'].to_numpy().astype('datetime64[M]')\n",
    "        \n",
    "        if self.monthly:\n",
    "            self.df = self.df.groupby(['TransDate', 'Wuc','lewdId','usmcsymbol','Start_Date_Time','Stop_Date_Time'])['daily_hours'].sum().reset_index()\n",
    "            self.df.dropna(inplace=True)\n",
    "            \n",
    "            self.df.rename({'daily_hours':'nmc_hours'},axis=1,inplace=True)\n",
    "            self.df['nmc_hours'] = round(self.df['nmc_hours'],2)\n",
    "            \n",
    "        else:\n",
    "            self.df = self.df.groupby(['lewdId','Evt_ID','Wuc','Start_Date_Time','Stop_Date_Time'])['daily_hours'].sum().reset_index()\n",
    "            \n",
    "            self.df.rename({'daily_hours':'nmc_hours'},axis=1,inplace=True)\n",
    "            self.df['nmc_hours'] = round(self.df['nmc_hours'],2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "## 2.1 - Deploy Rollup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#monthly rollup\n",
    "monthly = NMC_Calculation(df,'Lukestatusdetail2_Feb.csv','LukeOOR2_Feb.csv',monthly=True).df\n",
    "\n",
    "#lewd rollup\n",
    "# lewd = NMC_Calculation(df,'Lukestatusdetail.csv','LukeOOR.csv',monthly=False).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransDate</th>\n",
       "      <th>Wuc</th>\n",
       "      <th>lewdId</th>\n",
       "      <th>usmcsymbol</th>\n",
       "      <th>Start_Date_Time</th>\n",
       "      <th>Stop_Date_Time</th>\n",
       "      <th>nmc_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>56100111</td>\n",
       "      <td>5932-A171570030-1-1</td>\n",
       "      <td>NMC</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2021-10-01 09:00:00</td>\n",
       "      <td>582.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>56100226</td>\n",
       "      <td>5932-A171570031-1-1</td>\n",
       "      <td>NMC</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2021-10-01 08:00:00</td>\n",
       "      <td>582.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>56100111</td>\n",
       "      <td>5932-A171570030-1-1</td>\n",
       "      <td>NMC</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2021-10-01 09:00:00</td>\n",
       "      <td>744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>56100226</td>\n",
       "      <td>5932-A171570031-1-1</td>\n",
       "      <td>NMC</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2021-10-01 08:00:00</td>\n",
       "      <td>744.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>56100111</td>\n",
       "      <td>5932-A171570030-1-1</td>\n",
       "      <td>NMC</td>\n",
       "      <td>2017-06-06</td>\n",
       "      <td>2021-10-01 09:00:00</td>\n",
       "      <td>744.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransDate       Wuc               lewdId usmcsymbol Start_Date_Time  \\\n",
       "0 2017-06-01  56100111  5932-A171570030-1-1        NMC      2017-06-06   \n",
       "1 2017-06-01  56100226  5932-A171570031-1-1        NMC      2017-06-06   \n",
       "2 2017-07-01  56100111  5932-A171570030-1-1        NMC      2017-06-06   \n",
       "3 2017-07-01  56100226  5932-A171570031-1-1        NMC      2017-06-06   \n",
       "4 2017-08-01  56100111  5932-A171570030-1-1        NMC      2017-06-06   \n",
       "\n",
       "        Stop_Date_Time  nmc_hours  \n",
       "0  2021-10-01 09:00:00      582.9  \n",
       "1  2021-10-01 08:00:00      582.9  \n",
       "2  2021-10-01 09:00:00      744.0  \n",
       "3  2021-10-01 08:00:00      744.0  \n",
       "4  2021-10-01 09:00:00      744.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Read in and Filter Supply Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50235, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnxn_str = (\"Driver={SQL Server};\"\n",
    "            \"Server=NRS0752PXSQ07V\\DW02;\"\n",
    "            \"Database=WAREHOUSEDB2;\"\n",
    "           \"Trusted_Connection=yes;\")\n",
    "cnxn = pyodbc.connect(cnxn_str)\n",
    "\n",
    "# cursor = cnxn.cursor()\n",
    "\n",
    "# cursor.execute(\"declare @startdate as datetime;\")\n",
    "# cursor.execute(\"set @startdate = DATEADD(day, -5, Convert(date, getdate()))\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_sql(\"SELECT * FROM AIRFORCE.PartsOrdered;\", cnxn)\n",
    "#     df = pd.read_sql(\"SELECT * FROM AIRFORCE.AFPartsOrdered75months WHERE [TR DATE] BETWEEN '2016-01-01' AND '2017-01-01';\", cnxn)\n",
    "    del cnxn\n",
    "except:\n",
    "    del cnxn\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Parts_Ordered_Oct_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50235, 24)\n",
      "(25072, 24)\n"
     ]
    }
   ],
   "source": [
    "cols = {col:col.lower() for col in df.columns}\n",
    "df.rename(cols, axis=1, inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "pri_remove =['BQ','CZ','','AG','BG','BZ','CQ','BT','Y','JC','I','1C','1G','BC','N']\n",
    "df = df.loc[~(df['pri'].isin(pri_remove))]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - NMCS Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMCSCalculator():\n",
    "    '''\n",
    "    Determine NMCS hours using AF Supply Data.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, monthly = False):\n",
    "        self.df = df\n",
    "        self.jcn_sums = {}\n",
    "        \n",
    "        if monthly == True:\n",
    "            self.docDateExtract()\n",
    "            self.SiAssigns()\n",
    "            self.filterData()\n",
    "            self.retrieveRangeSums(keep_dates=True)\n",
    "            self.expandDaily()\n",
    "        else:\n",
    "            self.docDateExtract()\n",
    "            self.SiAssigns()\n",
    "            self.filterData()\n",
    "            self.retrieveRangeSums()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def docDateExtract(self):\n",
    "        \n",
    "        self.df['doc_date'] = self.df['document'].str[6:10].apply(lambda x: '202'+x if eval(x[0])<3 else '201'+x)\n",
    "\n",
    "        julian = self.df.doc_date.str[4:].str.extract(\"([1-9][0-9]?[0-9]?)\")    \n",
    "        self.df[\"doc_date\"] = self.df.doc_date.str[:4] + \"-\" + julian.iloc[:,0].astype(str)\n",
    "        self.df['doc_date'] = pd.to_datetime(self.df['doc_date'], errors='coerce', format='%Y-%j')\n",
    "        \n",
    "        null_docs = self.df.loc[self.df['doc_date'].isna()]['document'].unique()\n",
    "        \n",
    "        for doc in null_docs:\n",
    "            self.df.loc[(self.df['document']==doc) & (self.df['doc_date'].isnull()), 'doc_date'] = self.df.loc[self.df['document']==doc]['tr date'].min()\n",
    "         \n",
    "        self.df.loc[pd.to_datetime(self.df['doc_date']).dt.is_leap_year, 'doc_date'] = pd.to_datetime(self.df['doc_date']) - pd.DateOffset(days=1)\n",
    "        \n",
    "        odd_docs = self.df.loc[pd.to_datetime(self.df['doc_date']).dt.date > pd.to_datetime(self.df['tr date']).dt.date]['document'].unique()\n",
    "        \n",
    "        for doc in odd_docs:\n",
    "            self.df.loc[self.df['document']==doc, 'doc_date'] = self.df.loc[self.df['document']==doc]['tr date'].min()\n",
    "            \n",
    "            \n",
    "            \n",
    "    def SiAssigns(self):\n",
    "        #Assignning JCNs\n",
    "        \n",
    "        #turn empty jcn values to Nan values\n",
    "        self.df.loc[self.df['jcn'].str.strip()=='', 'jcn'] = np.nan\n",
    "        #fill our Nan values with the appropriate JCN\n",
    "        self.df.update(self.df.groupby('document')['jcn'].fillna(lambda x: x.fill().bfill()))\n",
    "        \n",
    "        #Assigning Mark-For\n",
    "        \n",
    "        #grab the first occurence of an 'A' in mark for - for each unique document\n",
    "        first = self.df.loc[self.df['mark for'].astype(str).str[2]=='A'].sort_values(self.df.columns.tolist())\\\n",
    "                  .drop_duplicates(subset=['document'], keep='first')\n",
    "        \n",
    "        #populate 1SI entries with 'A' mark for values\n",
    "        for i, row in first.iterrows():\n",
    "            self.df.loc[self.df['document']==row['document'], 'mark for'] = row['mark for']\n",
    "        \n",
    "       \n",
    "            \n",
    "            \n",
    "    \n",
    "    def filterData(self):\n",
    "        #remove all that are not 1SI or DOC, and remove all non-aircrafts if not AOO\n",
    "        self.df = self.df.loc[(self.df['tric'].isin(['1SI','DOC']))] \n",
    "        self.df = self.df.loc[(self.df['mark for'].str[2]=='A')]\n",
    "\n",
    "        #strip away white spaces from jcn\n",
    "        self.df['jcn'] = self.df['jcn'].astype(str).str.strip()\n",
    "        #remove erroneous jcn values\n",
    "        self.df = self.df.loc[self.df['jcn'].str[-1].isin(['1','2','3','4','5','6','7','8','9'])]\n",
    "        \n",
    "        #Drop Duplicates\n",
    "        self.df.drop_duplicates(subset=['jcn','doc_date','tr date'], inplace=True)\n",
    "        \n",
    "        \n",
    "        self.df.reset_index(inplace=True)\n",
    "        self.df.drop('index', axis=1,inplace=True)\n",
    "    \n",
    "        \n",
    "        \n",
    "    def createDocLog(self):\n",
    "        \n",
    "        \n",
    "        def hourConvert(row):\n",
    "            \n",
    "            minute_delta = pd.Timedelta(pd.to_datetime(row['tr date'])-pd.to_datetime(row['doc_date'])).total_seconds()\n",
    "            return minute_delta / 3600.0\n",
    "            \n",
    "        self.df['hours'] = np.where(pd.to_datetime(self.df['doc_date'])==pd.to_datetime(self.df['tr date']), 3, self.df.apply(hourConvert, axis=1))\n",
    "#                                     self.df.apply(lambda x:  pd.Timedelta(pd.datetime(x['tr date'])-pd.datetime(x['doc_date'])).total_seconds() / 3600.0))\n",
    "#                                     pd.Timedelta(pd.datetime(self.df['tr date'])-pd.datetime(self.df['doc_date'])).total_seconds() / 3600.0)\n",
    "        self.df['doc_date'] = pd.to_datetime(self.df['doc_date']).dt.date\n",
    "    \n",
    "    \n",
    "    def retrieveRangeSums(self, keep_dates=False):\n",
    "        \n",
    "        #filter to appropriate columns\n",
    "        self.df = self.df[['sran','jcn','doc_date','tr date']].sort_values(by=['jcn','doc_date'])\n",
    "        \n",
    "        #covert to datetime objects\n",
    "        self.df['doc_date'] = pd.to_datetime(self.df['doc_date']).dt.date\n",
    "        self.df['tr date'] = pd.to_datetime(self.df['tr date']).dt.date\n",
    "        \n",
    "        #create dict of each jcn possible date ranges\n",
    "        jcn_dict = {}\n",
    "\n",
    "        for i, row in self.df.iterrows():\n",
    "            if (row['jcn'], row['sran']) in jcn_dict:\n",
    "                jcn_dict[(row['jcn'], row['sran'])].append([row['doc_date'], row['tr date']])\n",
    "            else:\n",
    "                jcn_dict[(row['jcn'], row['sran'])] = [[row['doc_date'], row['tr date']]]\n",
    "                \n",
    "                \n",
    "                \n",
    "        #filter date ranges to the longest span\n",
    "        from datetime import datetime\n",
    "        from collections import namedtuple\n",
    "        \n",
    "        Range = namedtuple('Range', ['start', 'end'])\n",
    "        jcn_ranges = {}\n",
    "\n",
    "        for key, value in jcn_dict.items():\n",
    "            earliest_start, latest_end = value[0][0], value[0][1]\n",
    "            extra = []\n",
    "            for start, end in value[1:]:\n",
    "                if ((start <= latest_end) and (start >= earliest_start)) or ((end >= latest_end) and (end <= earliest_start)):\n",
    "                    r1 = Range(start=start, end=end)\n",
    "                    r2 = Range(start=earliest_start, end=latest_end)\n",
    "                    earliest_start = min(r1.start, r2.start)\n",
    "                    latest_end = max(r1.end, r2.end)\n",
    "                else:\n",
    "                    extra.append((start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')))\n",
    "                    earliest_start = start\n",
    "                    latest_end = end\n",
    "            jcn_ranges[key] = [(earliest_start.strftime('%Y-%m-%d'), latest_end.strftime('%Y-%m-%d'))]\n",
    "            if extra:\n",
    "                jcn_ranges[key].extend(extra)\n",
    "        jcn_ranges = {k:set(v) for k,v in jcn_ranges.items()}\n",
    "\n",
    "        \n",
    "        if keep_dates:            \n",
    "            self.df =pd.DataFrame.from_dict(jcn_ranges, orient='index').reset_index()\n",
    "            \n",
    "            # melt all date columns\n",
    "            max_dates = max([col for col in self.df.columns if str(col).isdigit()])\n",
    "            self.df = pd.melt(self.df, id_vars='index', value_vars=list(range(max_dates+1)), value_name='Dates')\n",
    "\n",
    "            # drop Null values\n",
    "            self.df.dropna(inplace=True)\n",
    "\n",
    "            #unzip jcn/sran and StartDate/EndDate\n",
    "            self.df['jcn'], self.df['sran'] = zip(*self.df['index'])\n",
    "            self.df['StartDate'], self.df['EndDate'] = zip(*self.df['Dates'])\n",
    "\n",
    "            #drop 'index', 'Dates' and 'variable' columns\n",
    "            self.df.drop(['index','variable','Dates'], axis=1, inplace=True)   \n",
    "            \n",
    "        else:\n",
    "            #get date deltas in hours based on each start date and end date pair\n",
    "            for key, val in jcn_ranges.items():\n",
    "                total_hours = 0\n",
    "                for start, end in val:\n",
    "                    if start==end:\n",
    "                        total_hours+=3\n",
    "                    else:\n",
    "                        hour_delta = pd.Timedelta(pd.to_datetime(end)-pd.to_datetime(start)).total_seconds() / 3600.0\n",
    "                        total_hours+=hour_delta\n",
    "                self.jcn_sums[key] = total_hours\n",
    "\n",
    "            self.df = pd.DataFrame(self.jcn_sums.items(), columns = ['jcn','nmcs_hours'])\n",
    "            self.df['jcn'], self.df['sran'] = zip(*self.df['jcn'])\n",
    "          \n",
    "\n",
    "            \n",
    "            \n",
    "    def expandDaily(self):\n",
    "        def drange(s, e):\n",
    "            s = str(s)\n",
    "            e = str(e)\n",
    "\n",
    "            return pd.Series(pd.date_range(s,e))\n",
    "\n",
    "        self.df['Date'] = [pd.date_range(s, e, freq='d') for s, e in\n",
    "              zip(pd.to_datetime(self.df['StartDate']), pd.to_datetime(self.df['EndDate']))]\n",
    "        \n",
    "        #expand rows for each day\n",
    "        self.df = self.df.explode('Date')\n",
    "        \n",
    "        def daily_hour(row):\n",
    "            if row['StartDate']==row['EndDate']:\n",
    "                return 3\n",
    "            elif row['Date']==row['EndDate']:\n",
    "                return 0\n",
    "            else:\n",
    "                return 24\n",
    "\n",
    "        self.df['daily_hours'] = self.df[['StartDate','EndDate','Date']].astype(str).apply(lambda x: daily_hour(x),axis=1)\n",
    "        \n",
    "        #get first of every month to roll up to when needed\n",
    "        self.df['TransDate'] = self.df['Date'].to_numpy().astype('datetime64[M]')\n",
    "        \n",
    "        #create final groupby object\n",
    "        self.df = self.df.groupby(['TransDate','jcn','sran'])['daily_hours'].sum().reset_index()\n",
    "        self.df.rename({'daily_hours':'nmcs_hours'},axis=1,inplace=True)\n",
    "        self.df['nmcs_hours'] = self.df['nmcs_hours'].astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "## 4.1 - Run NMCS Calculator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly = NMCSCalculator(df, monthly=True).df\n",
    "# df_lewd = NMCSCalculator(df).df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a name='5'></a>\n",
    "## 5 - Final Join and Rollup for NMC and NMCS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMCJoin:\n",
    "    '''\n",
    "    Join our NMC results with our NMCS results by lewd-Id and monthly.\n",
    "    '''\n",
    "    def __init__(self, nmc_df, nmcs_df, monthly = True):\n",
    "        self.nmc_df = nmc_df\n",
    "        self.nmcs_df = nmcs_df\n",
    "        self.monthly = monthly\n",
    "        \n",
    "        self.final_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "        self.NMCPrep()\n",
    "        self.NMCSPrep()\n",
    "        self.FinalRollup()\n",
    "    \n",
    "    \n",
    "    def NMCPrep(self):\n",
    "        '''\n",
    "        Prepare our NMC data for merger.\n",
    "        '''\n",
    "        \n",
    "        sran_map = {\n",
    "            '5417':'1801',\n",
    "            '5932':'1820',\n",
    "            '7070':'2316',\n",
    "            '5932':'4417',\n",
    "            '5955':'4469',\n",
    "            '5417':'4855',\n",
    "            '7070':'5209',\n",
    "            '5518':'5270',\n",
    "            '7476':'5518',\n",
    "            '5417':'multi-sran', #('7599','5807','5819','5897','5908','4804','5820','5806')\n",
    "            '5932':'multi-sran', #('7599','5807','5819','5897','5908','4804','5820','5806')\n",
    "            '7476':'multi-sran', #('7599','5807','5819','5897','5908','4804','5820','5806')\n",
    "            '5955':'6311'\n",
    "        }\n",
    "        \n",
    "        #sum all hours to ddr==1\n",
    "        self.nmc_df['non_ddr'] = self.nmc_df['lewdId'].str.rsplit('-', 1).str.get(0)\n",
    "        \n",
    "       \n",
    "    # if self.monthly:\n",
    "        #group our lewdID by ddr\n",
    "        non_ddr = self.nmc_df.groupby(['TransDate', 'Wuc','non_ddr','Start_Date_Time','Stop_Date_Time'])['nmc_hours'].sum().reset_index()\n",
    "\n",
    "         #filter to where that last of our lewdID (ddr) is 1\n",
    "        self.nmc_df = self.nmc_df.loc[self.nmc_df['lewdId'].str.endswith('1')]\n",
    "        self.nmc_df.drop('nmc_hours',axis=1,inplace=True)\n",
    "        self.nmc_df = pd.merge(self.nmc_df, non_ddr, how='left',on=['TransDate', 'Wuc','non_ddr','Start_Date_Time','Stop_Date_Time'])\n",
    "        self.nmc_df.drop('non_ddr',axis=1,inplace=True)\n",
    "    \n",
    "            \n",
    "# if-else this statement if needed\n",
    "            \n",
    "#             #group our lewdID by ddr\n",
    "#             non_ddr = self.nmc_df.groupby(['non_ddr'])['nmc_hours'].sum().reset_index()\n",
    "            \n",
    "#             #filter to where that last of our lewdID (ddr) is 1\n",
    "#             self.nmc_df = self.nmc_df.loc[self.nmc_df['lewdId'].str.endswith('1')]\n",
    "            \n",
    "            \n",
    "            \n",
    "        # get location of lewd to match to sran\n",
    "        self.nmc_df['location'] = self.nmc_df['lewdId'].str.split('-').str.get(0)\n",
    "\n",
    "        #get the sran from the location, to map to the nmcs\n",
    "        self.nmc_df['sran_map'] = self.nmc_df['location'].astype(str).map(sran_map) \n",
    "\n",
    "        #create jcn from lewd_id column\n",
    "        self.nmc_df['id'] = self.nmc_df['lewdId'].str[5:-2]\n",
    "        \n",
    "        #create evt_id column\n",
    "        self.nmc_df['evt_id'] = self.nmc_df['id'].str.split('-').str.get(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def NMCSPrep(self):\n",
    "        '''\n",
    "        Prepare our NMCS data for merger\n",
    "        '''\n",
    "        import string\n",
    "        import re\n",
    "\n",
    "        #since locations can map to multiple srans, let's change our srans to their appropriate list group\n",
    "        sran_group = ['7599','5807','5819','5897','5908','4804','5820','5806']\n",
    "        \n",
    "        #rename 'sran' to 'sran_map' for merging purposes\n",
    "        self.nmcs_df.rename({'sran':'sran_map'},axis=1,inplace=True)\n",
    "        \n",
    "        #translate our srans\n",
    "        self.nmcs_df.loc[self.nmcs_df['sran_map'].astype(str).isin(sran_group), 'sran_map'] = 'multi-sran'\n",
    "        \n",
    "        #getting rid of weird JCN entries\n",
    "        self.nmcs_df = self.nmcs_df.loc[~(self.nmcs_df['jcn'].str[-3:].str.contains('|'.join(string.ascii_lowercase), flags=re.IGNORECASE))]\n",
    "        \n",
    "        #create id from nmv column\n",
    "        self.nmcs_df['id'] = self.nmcs_df['jcn'].astype(str).apply(lambda x: x[:-3]+'-'+str(int(x[-3:])))\n",
    "\n",
    "        #turn sran col into str before merger\n",
    "        self.nmcs_df['sran_map'] = self.nmcs_df['sran_map'].astype(str)\n",
    "        \n",
    "        #create event id for merger\n",
    "        self.nmcs_df['evt_id'] = self.nmcs_df['id'].str.split('-').str.get(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def FinalRollup(self):\n",
    "        '''\n",
    "        Now that our dataframes are properly formatted, we can now join them and group by the appropriate field (month/lewd).\n",
    "        '''\n",
    "        #ensure transdates are the same datas type for merger\n",
    "        self.nmcs_df['TransDate'] = self.nmcs_df['TransDate'].astype(str)\n",
    "        self.nmc_df['TransDate'] = self.nmc_df['TransDate'].astype(str)\n",
    "        \n",
    "        \n",
    "        self.final_df = pd.merge(self.nmc_df, self.nmcs_df, on=['TransDate','evt_id','sran_map'], how='left')   \n",
    "\n",
    "         #if nmcs hours is greater than nmc hours, set nmcs hours to nmc hours\n",
    "        self.final_df['nmcs_hours'].fillna(0, inplace=True)\n",
    "        self.final_df['nmcs_hours'] = np.where(self.final_df['nmcs_hours']>self.final_df['nmc_hours'],self.final_df['nmc_hours'],self.final_df['nmcs_hours'])\n",
    "    \n",
    "        \n",
    "        if self.monthly:    \n",
    "\n",
    "            #final groupby statement (add back sran map)\n",
    "            self.final_df = (self.final_df.groupby(['TransDate','Wuc','lewdId','usmcsymbol','Start_Date_Time','Stop_Date_Time','evt_id','sran_map'])\n",
    "                           [['nmc_hours','nmcs_hours']]\n",
    "                           .sum()\n",
    "                           .reset_index())\n",
    "            \n",
    "    \n",
    "            # Rollup to month and Wuc\n",
    "            self.final_df = (self.final_df.groupby(['TransDate','Wuc','usmcsymbol'])[['nmc_hours','nmcs_hours']]\n",
    "                           .sum()\n",
    "                           .reset_index())\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "\n",
    "            #reorganize columns\n",
    "            self.final_df = self.final_df[['lewdId','evt_id','Wuc','usmcsymbol','Start_Date_Time','Stop_Date_Time', \n",
    "                                           'jcn','sran_map','location', 'nmc_hours','nmcs_hours']]\n",
    "\n",
    "            #group by lewdId\n",
    "            self.final_df = (self.final_df.groupby(['lewdId','evt_id','Wuc','usmcsymbol','Start_Date_Time','Stop_Date_Time',\n",
    "                                                     'sran_map','location'])[['nmc_hours','nmcs_hours']]\n",
    "                           .sum()\n",
    "                           .reset_index())\n",
    "\n",
    "            \n",
    "        #####FULL VERIONS#########\n",
    "#         #if nmcs hours is greater than nmc hours, set nmcs hours to nmc hours\n",
    "#         self.final_df['nmcs_hours'].fillna(0, inplace=True)\n",
    "#         self.final_df['nmcs_hours'] = np.where(self.final_df['nmcs_hours']>self.final_df['nmc_hours'],self.final_df['nmc_hours'],self.final_df['nmcs_hours'])\n",
    "#         print(self.final_df.nmc_hours.sum())\n",
    "        self.df = self.final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-1'></a>\n",
    "## 5.1 - Deploy Join Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke.Gray\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\Luke.Gray\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "monthly_rollup = NMCJoin(monthly, df_monthly, monthly=True).df\n",
    "lewd_rollup = NMCJoin(monthly, df_monthly, monthly=False).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finito\n"
     ]
    }
   ],
   "source": [
    "print('finito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_rollup.to_csv('2016_monthly_rollup.csv', index=False)\n",
    "# lewd_rollup.to_csv('2016_lewd_rollup.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "monthly = glob.glob('*monthly_rollup.csv')\n",
    "lewd = glob.glob('*lewd_rollup.csv')\n",
    "\n",
    "monthly_li, lewd_li = [],[]\n",
    "\n",
    "for file in monthly:\n",
    "    df = pd.read_csv(file)\n",
    "    monthly_li.append(df)\n",
    "    \n",
    "for file in lewd:\n",
    "    df = pd.read_csv(file)\n",
    "    lewd_li.append(df)\n",
    "    \n",
    "month_fin = pd.concat(monthly_li)\n",
    "lewd_fin = pd.concat(lewd_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot our nmc/pmc rows\n",
    "months = pd.pivot_table(monthly_rollup, values=['nmc_hours','nmcs_hours'], \n",
    "                        index=['TransDate','Wuc'], columns='usmcsymbol',\n",
    "                        aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten multi-index\n",
    "months.columns = months.columns.get_level_values(0)\n",
    "\n",
    "#rename columns\n",
    "months.columns.values[0] = \"nmc_hours\"\n",
    "months.columns.values[1] = \"pmc_hours\"\n",
    "months.columns.values[2] = \"nmcs_hours\"\n",
    "months.columns.values[3] = \"pmcs_hours\"\n",
    "\n",
    "months.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill Nan with 0\n",
    "months.fillna(0, inplace=True)\n",
    "\n",
    "#final_groupby\n",
    "months = (months.groupby(['TransDate','Wuc'])[['nmc_hours','pmc_hours','nmcs_hours','pmcs_hours']]\n",
    "          .sum()\n",
    "          .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransDate</th>\n",
       "      <th>Wuc</th>\n",
       "      <th>nmc_hours</th>\n",
       "      <th>pmc_hours</th>\n",
       "      <th>nmcs_hours</th>\n",
       "      <th>pmcs_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>281100</td>\n",
       "      <td>789.36</td>\n",
       "      <td>65.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>281401</td>\n",
       "      <td>460.46</td>\n",
       "      <td>65.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>2823AK</td>\n",
       "      <td>197.34</td>\n",
       "      <td>65.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>281100</td>\n",
       "      <td>864.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>281401</td>\n",
       "      <td>504.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TransDate     Wuc  nmc_hours  pmc_hours  nmcs_hours  pmcs_hours\n",
       "59   2021-09-01  281100     789.36      65.78         0.0         0.0\n",
       "76   2021-09-01  281401     460.46      65.78         0.0         0.0\n",
       "88   2021-09-01  2823AK     197.34      65.78         0.0         0.0\n",
       "164  2021-10-01  281100     864.00      72.00         0.0         0.0\n",
       "181  2021-10-01  281401     504.00      72.00         0.0         0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "months.loc[(months['nmc_hours']>0) & (months['pmc_hours']>0)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Lewd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot our nmc/pmc rows\n",
    "lewds = pd.pivot_table(lewd_rollup, values=['nmc_hours','nmcs_hours'], \n",
    "                        index=['lewdId','evt_id','Wuc','Start_Date_Time',\n",
    "                               'Stop_Date_Time','sran_map','location'], columns='usmcsymbol',\n",
    "                        aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten multi-index\n",
    "lewds.columns = lewds.columns.get_level_values(0)\n",
    "\n",
    "#rename columns\n",
    "lewds.columns.values[0] = \"nmc_hours\"\n",
    "lewds.columns.values[1] = \"pmc_hours\"\n",
    "lewds.columns.values[2] = \"nmcs_hours\"\n",
    "lewds.columns.values[3] = \"pmcs_hours\"\n",
    "\n",
    "lewds.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill Nan with 0\n",
    "lewds.fillna(0, inplace=True)\n",
    "\n",
    "#final_groupby\n",
    "lewds = (lewds.groupby(['lewdId','evt_id','Wuc','Start_Date_Time',\n",
    "                               'Stop_Date_Time','sran_map','location'])\n",
    "         [['nmc_hours','pmc_hours','nmcs_hours','pmcs_hours']]\n",
    "          .sum()\n",
    "          .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lewdId</th>\n",
       "      <th>evt_id</th>\n",
       "      <th>Wuc</th>\n",
       "      <th>Start_Date_Time</th>\n",
       "      <th>Stop_Date_Time</th>\n",
       "      <th>sran_map</th>\n",
       "      <th>location</th>\n",
       "      <th>nmc_hours</th>\n",
       "      <th>pmc_hours</th>\n",
       "      <th>nmcs_hours</th>\n",
       "      <th>pmcs_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>5417-A213080058-2-1</td>\n",
       "      <td>A213080058</td>\n",
       "      <td>052060000030A</td>\n",
       "      <td>2021-11-30 04:31:00</td>\n",
       "      <td>2021-12-07 11:27:48</td>\n",
       "      <td>multi-sran</td>\n",
       "      <td>5417</td>\n",
       "      <td>126.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11009</th>\n",
       "      <td>7070-C213360161-289-1</td>\n",
       "      <td>C213360161</td>\n",
       "      <td>2911SI</td>\n",
       "      <td>2021-11-21 21:15:00</td>\n",
       "      <td>2021-12-23 09:00:00</td>\n",
       "      <td>5209</td>\n",
       "      <td>7070</td>\n",
       "      <td>755.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5438</th>\n",
       "      <td>5932-A213220071-14-1</td>\n",
       "      <td>A213220071</td>\n",
       "      <td>6221130531</td>\n",
       "      <td>2021-11-24 21:10:00</td>\n",
       "      <td>2021-12-07 11:27:48</td>\n",
       "      <td>multi-sran</td>\n",
       "      <td>5932</td>\n",
       "      <td>302.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>5955-A220240103-3-1</td>\n",
       "      <td>A220240103</td>\n",
       "      <td>245001A7</td>\n",
       "      <td>2022-01-24 00:00:00</td>\n",
       "      <td>2022-01-24 18:00:00</td>\n",
       "      <td>6311</td>\n",
       "      <td>5955</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>7070-C213070125-1-1</td>\n",
       "      <td>C213070125</td>\n",
       "      <td>300000</td>\n",
       "      <td>2021-12-08 13:20:00</td>\n",
       "      <td>2021-12-11 12:20:00</td>\n",
       "      <td>5209</td>\n",
       "      <td>7070</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      lewdId      evt_id            Wuc     Start_Date_Time  \\\n",
       "1206     5417-A213080058-2-1  A213080058  052060000030A 2021-11-30 04:31:00   \n",
       "11009  7070-C213360161-289-1  C213360161         2911SI 2021-11-21 21:15:00   \n",
       "5438    5932-A213220071-14-1  A213220071     6221130531 2021-11-24 21:10:00   \n",
       "9714     5955-A220240103-3-1  A220240103       245001A7 2022-01-24 00:00:00   \n",
       "10199    7070-C213070125-1-1  C213070125         300000 2021-12-08 13:20:00   \n",
       "\n",
       "            Stop_Date_Time    sran_map location  nmc_hours  pmc_hours  \\\n",
       "1206   2021-12-07 11:27:48  multi-sran     5417     126.93        0.0   \n",
       "11009  2021-12-23 09:00:00        5209     7070     755.75        0.0   \n",
       "5438   2021-12-07 11:27:48  multi-sran     5932     302.28        0.0   \n",
       "9714   2022-01-24 18:00:00        6311     5955       1.98        0.0   \n",
       "10199  2021-12-11 12:20:00        5209     7070       0.00       71.0   \n",
       "\n",
       "       nmcs_hours  pmcs_hours  \n",
       "1206          0.0         0.0  \n",
       "11009         0.0         0.0  \n",
       "5438          0.0         0.0  \n",
       "9714          0.0         0.0  \n",
       "10199         0.0         0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lewds.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "months.to_csv('AFMonthlyRollup_Feb.csv',index=False)\n",
    "lewds.to_csv('AFLewdRollup_Feb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #put before roll up if needed       \n",
    "    ###fixing hours\n",
    "            #for those id's that apply to multiple lewd ids, use the sran to divvy up nmcs hours\n",
    "#             self.final_df['dup_zipped'] = [[i,j] for i,j in zip(self.final_df['evt_id'],self.final_df['sran_map'])]\n",
    "\n",
    "#             id_dups = self.final_df.loc[(self.final_df.groupby('evt_id')['lewdId'].transform('nunique').gt(1)) &\n",
    "#                                  (self.final_df['nmc_hours'].notnull())]['dup_zipped']\n",
    "            \n",
    "            \n",
    "#             #create groupby so we can reassign according to sran\n",
    "#             df_for_sran = self.nmcs_df.groupby(['evt_id','sran_map'])['nmcs_hours'].sum().reset_index()\n",
    "            \n",
    "#             for idx, sran in id_dups:\n",
    "#                 self.final_df.loc[(self.final_df['evt_id']==idx) & (self.final_df['sran_map']==sran), \n",
    "#                            'nmcs_hours'] = df_for_sran.loc[(df_for_sran['evt_id']==idx) & (df_for_sran['sran_map']==sran)]['nmcs_hours']\n",
    "                \n",
    "#             #drop unnecessary columns\n",
    "#             self.final_df.drop('dup_zipped', axis=1,inplace=True)\n",
    "            #####fin fixing hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
